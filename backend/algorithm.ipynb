{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2592</td>\n",
       "      <td>0</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18359</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1040</td>\n",
       "      <td>0</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17262</td>\n",
       "      <td>1</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9908</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  labels                                               text\n",
       "0   2592       0  Un-bleeping-believable! Meg Ryan doesn't even ...\n",
       "1  18359       1  This is a extremely well-made film. The acting...\n",
       "2   1040       0  Every once in a long while a movie will come a...\n",
       "3  17262       1  Name just says it all. I watched this movie wi...\n",
       "4   9908       0  This movie succeeds at being one of the most u..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data\\\\imdb_raw_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['id', 'text']]\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_paragraph(para):\n",
    "    lmtzr = stem.WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    p = ' '.join([lmtzr.lemmatize(token.lower()) for token in tokenizer.tokenize(para)])\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.087932109832764\n"
     ]
    }
   ],
   "source": [
    "import time; t0 = time.time()\n",
    "\n",
    "data['text'] = [clean_paragraph(para) for para in data['text']]\n",
    "\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build TfIDF Vector on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x67882 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3373863 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer().fit_transform(data['text'])\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**\n",
    "We would like to compute a score between a query term t and a\n",
    "document d, based on the weight of t in d.\n",
    "\n",
    "TERM FREQUENCY - The simplest approach is to assign the weight to be equal to the number of occurrences of term t in document d. This weighting scheme is referred to as term frequency and is denoted tf(t,d)\n",
    "\n",
    "In this view of a document, known in the literature as the bag\n",
    "of words model, the exact ordering of the terms in a document is ignored but the number of occurrences of each term is material\n",
    "\n",
    "An immediate idea is to scale down the term weights of terms with high collection frequency.\n",
    "\n",
    "**\n",
    "DOCUMENT FREQUENCY - df(t) - Defined to be the number of documents in the collection that contain a term t.\n",
    "\n",
    "// it is better to use a document-level statistic (such as the number of documents containing a term) than to use a collection-wide statistic for the term.\n",
    "\n",
    "**\n",
    "INVERSE DOCUMENT FREQUENCY (idf) of a term t as follows:\n",
    "idf(t) = log(N/df(t))\n",
    "\n",
    "//Thus the idf of a rare term is high, whereas the idf of a frequent term is likely to be low.\n",
    "\n",
    "\n",
    "**\n",
    "The TF-IDF weighting scheme assigns to term t a weight in document d given by:\n",
    "\n",
    "tf-idf(t,d) = tf(t,d) × idf(t)\n",
    "\n",
    "In other words, tf-idf(t,d) assigns to term t a weight in document d that is\n",
    "1. highest when t occurs many times within a small number of documents (thus lending high discriminating power to those documents)\n",
    "2. lower when the term occurs fewer times in a document, or occurs in many documents (thus offering a less pronounced relevance signal)\n",
    "3. lowest when the term occurs in virtually all documents.\n",
    "\n",
    "**\n",
    "We may view each document as a vector with one component corresponding to each term in the dictionary, together with a weight for each component.\n",
    "\n",
    "A document may have a high cosine score for a query even if it does not contain all query terms.\n",
    "\n",
    "Computing the cosine similarities between the query vector and each document vector in the collection, sorting the resulting scores and selecting the top K documents can be expensive.\n",
    "\n",
    "**\n",
    "COSINESCORE(q)\n",
    "1 float Scores[N] = 0\n",
    "2 Initialize Length[N]\n",
    "3 for each query term t\n",
    "4 do calculate wt,q and fetch postings list for t\n",
    "    5 for each pair(d, tft,d) in postings list\n",
    "    6 do Scores[d] += wft,d × wt,q\n",
    "7 Read the array Length[d]\n",
    "8 for each d\n",
    "9 do Scores[d] = Scores[d]/Length[d]\n",
    "10 return Top K components of Scores[]\n",
    "\n",
    "**\n",
    "First, if we are using inverse document frequency, we need not precompute idf(t)\n",
    "1. It suffices to store N/df(t) at the head of the\n",
    "postings for t.\n",
    "2. We store the term frequency tf(t,d) for each postings entry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
